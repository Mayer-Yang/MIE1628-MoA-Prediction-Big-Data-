{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql.functions import explode, col, udf, mean as _mean, stddev as _stddev, log, log10\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf, expr, concat, col, count, when, isnan\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "sqlc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data set\t\n",
    "train_features = sqlc.read.csv('/FileStore/tables/train_features.csv', header = True,inferSchema=True)\n",
    "train_targets_scored = sqlc.read.csv('/FileStore/tables/train_targets_scored.csv', header = True,inferSchema=True)\n",
    "\n",
    "test_features = sqlc.read.csv('/FileStore/tables/test_features.csv', header = True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define error function \n",
    "def error (prediction,target):\n",
    "  add_prob = udf(lambda x: x[1].item(), FloatType())\n",
    "  add_1_prob = udf(lambda x: x[0].item(), FloatType())\n",
    "  prediction = prediction.withColumn('log_prob',F.log(add_prob('probability')))\n",
    "  prediction = prediction.withColumn('log_(1-prob)',F.log(add_1_prob('probability')))\n",
    "  prediction = prediction.withColumn('loss', -F.col('label')*F.col('log_prob')-(1.-F.col('label'))*F.col('log_(1-prob)'))\n",
    "  \n",
    "  \n",
    "  loss = prediction.agg(F.mean('loss')).collect()[0]['avg(loss)']\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "def LR (train_features, train_targets_scored, test_features):\n",
    "    \n",
    "    tot_loss = 0\n",
    "    tot_loss_test = 0\n",
    "    predicted = []\n",
    "    for iterator in range(1,len(train_targets_scored.columns)-1): # logistic regression of each target \n",
    "        #add id to target\n",
    "        targetdf = train_targets_scored.select(train_targets_scored.columns[0],train_targets_scored.columns[iterator])\n",
    "        #merge features and target by id \n",
    "        targetname = train_targets_scored.columns[iterator]\n",
    "        df_all = train_features.join(targetdf, \"sig_id\", \"inner\")\n",
    "        df_all=df_all.drop(\"sig_id\")\n",
    "  \n",
    "        #split data to train/validation sets     \n",
    "        df_all.groupBy(targetname).count().show()\n",
    "        df = df_all.sampleBy(targetname,fractions={0: 0.7, 1: 0.7},seed=99)\n",
    "        df_test = df_all.subtract(df)\n",
    "        #df.groupBy(targetname).count().show()\n",
    "        #df_test.groupBy(targetname).count().show()\n",
    "        print('done splitting')\n",
    "        \n",
    "        #get heaters for all features    \n",
    "        cols = df.columns\n",
    "        #Define the features to be encoded  \n",
    "        categoricalColumns = ['cp_type', 'cp_time', 'cp_dose']\n",
    "        stages = []\n",
    "        #encoding categorical features \n",
    "        for categoricalCol in categoricalColumns:\n",
    "            stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "            encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "            stages += [stringIndexer, encoder]\n",
    "        #merge the encoded categorical features with all other features \n",
    "        label_stringIdx = StringIndexer(inputCol = train_targets_scored.columns[iterator], outputCol = 'label')\n",
    "        stages += [label_stringIdx]\n",
    "        numericCols = train_features.columns[4:]\n",
    "        assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "        assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "        stages += [assembler]\n",
    "        \n",
    "        #use pipeline to prepare input data for trainning/validation \t\t\n",
    "        pipeline = Pipeline(stages = stages)\n",
    "        pipelineModel = pipeline.fit(df)\n",
    "        df = pipelineModel.transform(df)\n",
    "        selectedCols = ['label', 'features'] + cols\n",
    "        df = df.select(selectedCols)\n",
    "        #df.printSchema()\n",
    "        print(\"train input ready\")\n",
    "        \n",
    "        pipelineModel = pipeline.fit(df_test)\n",
    "        df_test = pipelineModel.transform(df_test)\n",
    "        df_test = df_test.select(selectedCols)\n",
    "        #df_test.printSchema()\n",
    "        print(\"test input ready\")\n",
    "        \n",
    "        '''\n",
    "        train with deflut setting except maxIter=10 to acc\n",
    "        hyperparameters: \n",
    "        elasticNetParam = 0 \n",
    "        maxIter = 10\n",
    "        regParam = 0\n",
    "        standardization = True\n",
    "        tol = 1e-6\n",
    "        weightCol = 1.0 for all features \n",
    "        '''\n",
    "        lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "        # Fit the model\n",
    "        lrModel = lr.fit(df)\n",
    "        # make predictions and evaluate\n",
    "        predictions = lrModel.transform(df)\n",
    "        predictions_test = lrModel.transform(df_test)\n",
    "        predicted.append(predictions_test)\n",
    "        \n",
    "        tot_loss += error(predictions,df)\n",
    "        tot_loss_test += error(predictions_test)        \n",
    "        \n",
    "        print(\"target #\", iterator, train_targets_scored.columns[iterator])\n",
    "        print(\"Current mean total loss:\", tot_loss/iterator)\n",
    "        print(\"Current mean total loss (test):\", tot_loss_test/iterator)\n",
    "\n",
    "    tot_loss = tot_loss/len(train_targets_scored.columns)\n",
    "    tot_loss_test = tot_loss_test/len(train_targets_scored.columns)\n",
    "    \n",
    "    return predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
